{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from dotenv import load_dotenv  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector  \n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    " \n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = \"gpt-35-turbo-0613\"\n",
    "OPENAI_MODEL_NAME = \"gpt-35-turbo-0613\"\n",
    "OPENAI_DEPLOYMENT_VERSION = \"2023-07-01-preview\"\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "#---\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "COGNITIVE_SEARCH_INDEX_NAME = \"cognitive-search-vectordb-index_v1\"\n",
    "search_client = SearchClient(service_endpoint, index_name=COGNITIVE_SEARCH_INDEX_NAME, credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Document Embeddings using OpenAI Ada 002\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def generate_embeddings(page):\n",
    "    response = openai.Embedding.create(\n",
    "        input=page, engine=\"text-embedding-ada-002\")\n",
    "   \n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions (for calling); search in azure cognitive search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(topic, query, topic_filter):\n",
    "    \n",
    "    print (\"In function: get_info...\\n\")\n",
    "    print (f\"topic: {topic}\\n\")\n",
    "    print (f\"query: {query}\\n\")\n",
    "    print (f\"topic_filter: {topic_filter}\\n\")\n",
    "    \n",
    "    results = search_client.search(\n",
    "    search_text=query,\n",
    "    #filter=topic_filter,\n",
    "    vector=generate_embeddings(query), top_k=3,  \n",
    "    vector_fields=\"textVector\",\n",
    "    query_type=\"semantic\",\n",
    "    query_language=\"en-us\",\n",
    "    semantic_configuration_name='vectordb-semantic-config',\n",
    "    query_caption=\"extractive\",\n",
    "    query_answer=\"extractive\",\n",
    "    select=[\"source\", \"text\"],\n",
    "    top=3)\n",
    "    \n",
    "\n",
    "    semantic_answers = results.get_answers()\n",
    "    semantic_prompt = \"\"\n",
    "    \n",
    "    for answer in semantic_answers:\n",
    "        if answer.highlights:\n",
    "            semantic_prompt += f\"topic: {topic}; topic_text: {answer.highlights}\\n\"\n",
    "        else:\n",
    "            semantic_prompt += f\"topic: {topic }; topic_text: {answer.text}\\n\"\n",
    "            \n",
    "    print (f\"final semantic prompt: {semantic_prompt}\")\n",
    "    \n",
    "    query_prompt = \"\"\n",
    "    \n",
    "    for result in results:\n",
    "        query_prompt += f\"Text: {result['text']}\\n\"\n",
    "        \n",
    "    print (f\"final query prompt: {query_prompt}\")\n",
    "    \n",
    "    if semantic_prompt:\n",
    "        return semantic_prompt\n",
    "    return query_prompt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\" Assistant is a large language model designed to help employees of a company to find answers to their questions for various \n",
    "technical topics.\n",
    "You have access to an Azure Cognitive Search index that contains a large amount of technical documentation on different topics.\n",
    "You can search for the asked information in the index and return the most relevant results to the user. \n",
    "The index contains information about the following topics: Semantic Kernel, Langchain, and others.\n",
    "You are designed to be an interactive assistant, so you can ask the user for more information if needed to find the most relevant answer.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"Which programing languages are supported by semantic kernel?\"}]\n",
    "    \n",
    "functions = [\n",
    "        {\n",
    "            \"name\": \"get_info\",\n",
    "            \"description\": \"get detailed information about asked topic from Azure Cognitive Search\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"topic\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"the domain of the question being asked, such as the semantic kernel, langchain, etc.\",\n",
    "                    },\n",
    "                    \"query\": {\n",
    "                        \"type\" : \"string\",\n",
    "                        \"description\": \"the question that was asked\",\n",
    "                        \n",
    "                    },\n",
    "                    \"topic_filter\": {\n",
    "                        \"type\" : \"string\",\n",
    "                        \"description\": \"\"\"The filter to apply for the topic. Generate the filter using the following format: topic/any(i: i eq 'Semantic Kernel')\n",
    "                        If you are not sure about the topic, generate empty filter: topic/any(i: i eq '')\"\"\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"topic\",\"query\",\"topic_filter\"],\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "available_functions = {\n",
    "            \"get_info\": get_info,\n",
    "        } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "        deployment_id = \"gpt-35-turbo-0613\",\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        temperature=0.2,\n",
    "        function_call=\"auto\", \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7pDy7RIs82p2oKAtnIk0VoyLcPLqM at 0x13c5a20c0> JSON: {\n",
       "  \"id\": \"chatcmpl-7pDy7RIs82p2oKAtnIk0VoyLcPLqM\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1692442779,\n",
       "  \"model\": \"gpt-35-turbo\",\n",
       "  \"prompt_annotations\": [\n",
       "    {\n",
       "      \"prompt_index\": 0,\n",
       "      \"content_filter_results\": {\n",
       "        \"hate\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"self_harm\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"sexual\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"violence\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"function_call\",\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"function_call\": {\n",
       "          \"name\": \"get_info\",\n",
       "          \"arguments\": \"{\\n  \\\"topic\\\": \\\"Semantic Kernel\\\",\\n  \\\"query\\\": \\\"supported programming languages\\\",\\n  \\\"topic_filter\\\": \\\"topic/any(i: i eq 'Semantic Kernel')\\\"\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"content_filter_results\": {}\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 42,\n",
       "    \"prompt_tokens\": 258,\n",
       "    \"total_tokens\": 300\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display(JSON(response))\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting all parts togther\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qna(messages, functions, available_functions):\n",
    "    \n",
    "    #1. Ask GPT to find matching function\n",
    "    response = openai.ChatCompletion.create(\n",
    "        deployment_id = \"gpt-35-turbo-0613\",\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        temperature=0.2,\n",
    "        function_call=\"auto\", \n",
    "    )\n",
    "    \n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "    \n",
    "    #2 check if the model found a function\n",
    "    if response_message.get(\"function_call\"):\n",
    "        print(\"Recommended function:\")\n",
    "        print(response_message.get(\"function_call\"))\n",
    "        \n",
    "        #3. Run the function\n",
    "        function_name = response_message[\"function_call\"][\"name\"]\n",
    "        if function_name not in available_functions:\n",
    "            return \"Function \" + function_name + \" does not exist\"\n",
    "        function_to_call = available_functions[function_name] \n",
    "        \n",
    "        # verify function has correct number of arguments\n",
    "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "        print (f\"Function arguments:{function_args}\")\n",
    "        function_response = function_to_call(**function_args) \n",
    "        \n",
    "        print(\"Output of function call:\")\n",
    "        print(function_response)\n",
    "        print()\n",
    "        #4. Ask GPT to find matching function\n",
    "        \n",
    "        # adding assistant response to messages\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": response_message[\"role\"],\n",
    "                \"name\": response_message[\"function_call\"][\"name\"],\n",
    "                \"content\": response_message[\"function_call\"][\"arguments\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # adding function response to messages\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )  # extend conversation with function response\n",
    "        \n",
    "        print(\"Messages in second request:\")\n",
    "        for message in messages:\n",
    "            print(message)\n",
    "        print()\n",
    "        \n",
    "        second_response = openai.ChatCompletion.create(\n",
    "            messages=messages,\n",
    "            deployment_id= \"gpt-35-turbo-0613\"\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "        return second_response\n",
    "    else:\n",
    "        print(\"No function call found\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended function:\n",
      "{\n",
      "  \"name\": \"get_info\",\n",
      "  \"arguments\": \"{\\n  \\\"topic\\\": \\\"Semantic Kernel\\\",\\n  \\\"query\\\": \\\"supported programming languages\\\",\\n  \\\"topic_filter\\\": \\\"topic/any(i: i eq 'Semantic Kernel')\\\"\\n}\"\n",
      "}\n",
      "Function arguments:{'topic': 'Semantic Kernel', 'query': 'supported programming languages', 'topic_filter': \"topic/any(i: i eq 'Semantic Kernel')\"}\n",
      "In function: get_info...\n",
      "\n",
      "topic: Semantic Kernel\n",
      "\n",
      "query: supported programming languages\n",
      "\n",
      "topic_filter: topic/any(i: i eq 'Semantic Kernel')\n",
      "\n",
      "final semantic prompt: \n",
      "final query prompt: Text: Source code for langchain.document_loaders.parsers.language.language_parser\n",
      "from typing import Any, Dict, Iterator, Optional\n",
      "from langchain.docstore.document import Document\n",
      "from langchain.document_loaders.base import BaseBlobParser\n",
      "from langchain.document_loaders.blob_loaders import Blob\n",
      "from langchain.document_loaders.parsers.language.javascript import JavaScriptSegmenter\n",
      "from langchain.document_loaders.parsers.language.python import PythonSegmenter\n",
      "from langchain.text_splitter import Language\n",
      "LANGUAGE_EXTENSIONS: Dict[str, str] = {\n",
      "    \"py\": Language.PYTHON,\n",
      "    \"js\": Language.JS,\n",
      "}\n",
      "LANGUAGE_SEGMENTERS: Dict[str, Any] = {\n",
      "    Language.PYTHON: PythonSegmenter,\n",
      "    Language.JS: JavaScriptSegmenter,\n",
      "}\n",
      "[docs]class LanguageParser(BaseBlobParser):\n",
      "    \"\"\"\n",
      "    Language parser that split code using the respective language syntax.\n",
      "    Each top-level function and class in the code is loaded into separate documents.\n",
      "    Furthermore, an extra document is generated, containing the remaining top-level code\n",
      "    that excludes the already segmented functions and classes.\n",
      "    This approach can potentially improve the accuracy of QA models over source code.\n",
      "    Currently, the supported languages for code parsing are Python and JavaScript.\n",
      "    The language used for parsing can be configured, along with the minimum number of\n",
      "    lines required to activate the splitting based on syntax.\n",
      "    Examples:\n",
      "        .. code-block:: python\n",
      "            from langchain.text_splitter.Language\n",
      "            from langchain.document_loaders.generic import GenericLoader\n",
      "            from langchain.document_loaders.parsers import LanguageParser\n",
      "            loader = GenericLoader.from_filesystem(\n",
      "                \"./code\",\n",
      "                glob=\"**/*\",\n",
      "                suffixes=[\".py\", \".js\"],\n",
      "                parser=LanguageParser()\n",
      "            )\n",
      "            docs = loader.load()\n",
      "        Example instantiations to manually select the language:\n",
      "Text: langchain.document_loaders.parsers.language.language_parser.LanguageParser¶\n",
      "class langchain.document_loaders.parsers.language.language_parser.LanguageParser(language: Optional[Language] = None, parser_threshold: int = 0)[source]¶\n",
      "Language parser that split code using the respective language syntax.\n",
      "Each top-level function and class in the code is loaded into separate documents.\n",
      "Furthermore, an extra document is generated, containing the remaining top-level code\n",
      "that excludes the already segmented functions and classes.\n",
      "This approach can potentially improve the accuracy of QA models over source code.\n",
      "Currently, the supported languages for code parsing are Python and JavaScript.\n",
      "The language used for parsing can be configured, along with the minimum number of\n",
      "lines required to activate the splitting based on syntax.\n",
      "Examples\n",
      "from langchain.text_splitter.Language\n",
      "from langchain.document_loaders.generic import GenericLoader\n",
      "from langchain.document_loaders.parsers import LanguageParser\n",
      "loader = GenericLoader.from_filesystem(\n",
      "    \"./code\",\n",
      "    glob=\"**/*\",\n",
      "    suffixes=[\".py\", \".js\"],\n",
      "    parser=LanguageParser()\n",
      ")\n",
      "docs = loader.load()\n",
      "Example instantiations to manually select the language:\n",
      "… code-block:: python\n",
      "from langchain.text_splitter import Language\n",
      "loader = GenericLoader.from_filesystem(“./code”,\n",
      "glob=”**/*”,\n",
      "suffixes=[“.py”],\n",
      "parser=LanguageParser(language=Language.PYTHON)\n",
      ")\n",
      "Example instantiations to set number of lines threshold:\n",
      "… code-block:: python\n",
      "loader = GenericLoader.from_filesystem(“./code”,\n",
      "glob=”**/*”,\n",
      "suffixes=[“.py”],\n",
      "parser=LanguageParser(parser_threshold=200)\n",
      ")\n",
      "Language parser that split code using the respective language syntax.\n",
      "Parameters\n",
      "language – If None (default), it will try to infer language from source.\n",
      "Text: Step Component Descr iption\n",
      "that have already been loaded into the kernel to create additional steps.\n",
      "This is similar to how ChatGPT, Bing, and Microsoft 365 Copilot combines\n",
      "plugins together in their experiences.\n",
      "2.3 Connectors To get additional data or to perform autonomous actions, you can use out-\n",
      "of-the-box plugins like the Microsoft Graph Connector kit or create a\n",
      "custom connector to provide data to your own services.\n",
      "2.4 Custom\n",
      "pluginsAs a developer, you can create custom plugins that run inside of Semantic\n",
      "Kernel. These plugins can consist of either LLM prompts (semantic\n",
      "functions) or native C# or Python code (native function). This allows you to\n",
      "add new AI capabilities and integrate your existing apps and services into\n",
      "Semantic K ernel.\n",
      "3 Response Once the kernel is done, you can send the response back to the user to let\n",
      "them know the process is complete.\n",
      "To make sure all developers can take advantage of our learnings building Copilots, we\n",
      "have released Semantic K ernel as an open-source project  on GitHub. T oday, we\n",
      "provide the SDK in .NET and Python flavors (T ypescript and Java are coming soon). For a\n",
      "full list of what is supported in each language, see supported languages .\n",
      "Semantic Kernel is open-source\n",
      "\n",
      "Output of function call:\n",
      "Text: Source code for langchain.document_loaders.parsers.language.language_parser\n",
      "from typing import Any, Dict, Iterator, Optional\n",
      "from langchain.docstore.document import Document\n",
      "from langchain.document_loaders.base import BaseBlobParser\n",
      "from langchain.document_loaders.blob_loaders import Blob\n",
      "from langchain.document_loaders.parsers.language.javascript import JavaScriptSegmenter\n",
      "from langchain.document_loaders.parsers.language.python import PythonSegmenter\n",
      "from langchain.text_splitter import Language\n",
      "LANGUAGE_EXTENSIONS: Dict[str, str] = {\n",
      "    \"py\": Language.PYTHON,\n",
      "    \"js\": Language.JS,\n",
      "}\n",
      "LANGUAGE_SEGMENTERS: Dict[str, Any] = {\n",
      "    Language.PYTHON: PythonSegmenter,\n",
      "    Language.JS: JavaScriptSegmenter,\n",
      "}\n",
      "[docs]class LanguageParser(BaseBlobParser):\n",
      "    \"\"\"\n",
      "    Language parser that split code using the respective language syntax.\n",
      "    Each top-level function and class in the code is loaded into separate documents.\n",
      "    Furthermore, an extra document is generated, containing the remaining top-level code\n",
      "    that excludes the already segmented functions and classes.\n",
      "    This approach can potentially improve the accuracy of QA models over source code.\n",
      "    Currently, the supported languages for code parsing are Python and JavaScript.\n",
      "    The language used for parsing can be configured, along with the minimum number of\n",
      "    lines required to activate the splitting based on syntax.\n",
      "    Examples:\n",
      "        .. code-block:: python\n",
      "            from langchain.text_splitter.Language\n",
      "            from langchain.document_loaders.generic import GenericLoader\n",
      "            from langchain.document_loaders.parsers import LanguageParser\n",
      "            loader = GenericLoader.from_filesystem(\n",
      "                \"./code\",\n",
      "                glob=\"**/*\",\n",
      "                suffixes=[\".py\", \".js\"],\n",
      "                parser=LanguageParser()\n",
      "            )\n",
      "            docs = loader.load()\n",
      "        Example instantiations to manually select the language:\n",
      "Text: langchain.document_loaders.parsers.language.language_parser.LanguageParser¶\n",
      "class langchain.document_loaders.parsers.language.language_parser.LanguageParser(language: Optional[Language] = None, parser_threshold: int = 0)[source]¶\n",
      "Language parser that split code using the respective language syntax.\n",
      "Each top-level function and class in the code is loaded into separate documents.\n",
      "Furthermore, an extra document is generated, containing the remaining top-level code\n",
      "that excludes the already segmented functions and classes.\n",
      "This approach can potentially improve the accuracy of QA models over source code.\n",
      "Currently, the supported languages for code parsing are Python and JavaScript.\n",
      "The language used for parsing can be configured, along with the minimum number of\n",
      "lines required to activate the splitting based on syntax.\n",
      "Examples\n",
      "from langchain.text_splitter.Language\n",
      "from langchain.document_loaders.generic import GenericLoader\n",
      "from langchain.document_loaders.parsers import LanguageParser\n",
      "loader = GenericLoader.from_filesystem(\n",
      "    \"./code\",\n",
      "    glob=\"**/*\",\n",
      "    suffixes=[\".py\", \".js\"],\n",
      "    parser=LanguageParser()\n",
      ")\n",
      "docs = loader.load()\n",
      "Example instantiations to manually select the language:\n",
      "… code-block:: python\n",
      "from langchain.text_splitter import Language\n",
      "loader = GenericLoader.from_filesystem(“./code”,\n",
      "glob=”**/*”,\n",
      "suffixes=[“.py”],\n",
      "parser=LanguageParser(language=Language.PYTHON)\n",
      ")\n",
      "Example instantiations to set number of lines threshold:\n",
      "… code-block:: python\n",
      "loader = GenericLoader.from_filesystem(“./code”,\n",
      "glob=”**/*”,\n",
      "suffixes=[“.py”],\n",
      "parser=LanguageParser(parser_threshold=200)\n",
      ")\n",
      "Language parser that split code using the respective language syntax.\n",
      "Parameters\n",
      "language – If None (default), it will try to infer language from source.\n",
      "Text: Step Component Descr iption\n",
      "that have already been loaded into the kernel to create additional steps.\n",
      "This is similar to how ChatGPT, Bing, and Microsoft 365 Copilot combines\n",
      "plugins together in their experiences.\n",
      "2.3 Connectors To get additional data or to perform autonomous actions, you can use out-\n",
      "of-the-box plugins like the Microsoft Graph Connector kit or create a\n",
      "custom connector to provide data to your own services.\n",
      "2.4 Custom\n",
      "pluginsAs a developer, you can create custom plugins that run inside of Semantic\n",
      "Kernel. These plugins can consist of either LLM prompts (semantic\n",
      "functions) or native C# or Python code (native function). This allows you to\n",
      "add new AI capabilities and integrate your existing apps and services into\n",
      "Semantic K ernel.\n",
      "3 Response Once the kernel is done, you can send the response back to the user to let\n",
      "them know the process is complete.\n",
      "To make sure all developers can take advantage of our learnings building Copilots, we\n",
      "have released Semantic K ernel as an open-source project  on GitHub. T oday, we\n",
      "provide the SDK in .NET and Python flavors (T ypescript and Java are coming soon). For a\n",
      "full list of what is supported in each language, see supported languages .\n",
      "Semantic Kernel is open-source\n",
      "\n",
      "\n",
      "Messages in second request:\n",
      "{'role': 'system', 'content': ' Assistant is a large language model designed to help employees of a company to find answers to their questions for various \\ntechnical topics.\\nYou have access to an Azure Cognitive Search index that contains a large amount of technical documentation on different topics.\\nYou can search for the asked information in the index and return the most relevant results to the user. \\nThe index contains information about the following topics: Semantic Kernel, Langchain, and others.\\nYou are designed to be an interactive assistant, so you can ask the user for more information if needed to find the most relevant answer.\\n'}\n",
      "{'role': 'user', 'content': 'Which programing languages are supported by semantic kernel?'}\n",
      "{'role': 'assistant', 'name': 'get_info', 'content': '{\\n  \"topic\": \"Semantic Kernel\",\\n  \"query\": \"supported programming languages\",\\n  \"topic_filter\": \"topic/any(i: i eq \\'Semantic Kernel\\')\"\\n}'}\n",
      "{'role': 'function', 'name': 'get_info', 'content': 'Text: Source code for langchain.document_loaders.parsers.language.language_parser\\nfrom typing import Any, Dict, Iterator, Optional\\nfrom langchain.docstore.document import Document\\nfrom langchain.document_loaders.base import BaseBlobParser\\nfrom langchain.document_loaders.blob_loaders import Blob\\nfrom langchain.document_loaders.parsers.language.javascript import JavaScriptSegmenter\\nfrom langchain.document_loaders.parsers.language.python import PythonSegmenter\\nfrom langchain.text_splitter import Language\\nLANGUAGE_EXTENSIONS: Dict[str, str] = {\\n    \"py\": Language.PYTHON,\\n    \"js\": Language.JS,\\n}\\nLANGUAGE_SEGMENTERS: Dict[str, Any] = {\\n    Language.PYTHON: PythonSegmenter,\\n    Language.JS: JavaScriptSegmenter,\\n}\\n[docs]class LanguageParser(BaseBlobParser):\\n    \"\"\"\\n    Language parser that split code using the respective language syntax.\\n    Each top-level function and class in the code is loaded into separate documents.\\n    Furthermore, an extra document is generated, containing the remaining top-level code\\n    that excludes the already segmented functions and classes.\\n    This approach can potentially improve the accuracy of QA models over source code.\\n    Currently, the supported languages for code parsing are Python and JavaScript.\\n    The language used for parsing can be configured, along with the minimum number of\\n    lines required to activate the splitting based on syntax.\\n    Examples:\\n        .. code-block:: python\\n            from langchain.text_splitter.Language\\n            from langchain.document_loaders.generic import GenericLoader\\n            from langchain.document_loaders.parsers import LanguageParser\\n            loader = GenericLoader.from_filesystem(\\n                \"./code\",\\n                glob=\"**/*\",\\n                suffixes=[\".py\", \".js\"],\\n                parser=LanguageParser()\\n            )\\n            docs = loader.load()\\n        Example instantiations to manually select the language:\\nText: langchain.document_loaders.parsers.language.language_parser.LanguageParser¶\\nclass langchain.document_loaders.parsers.language.language_parser.LanguageParser(language: Optional[Language] = None, parser_threshold: int = 0)[source]¶\\nLanguage parser that split code using the respective language syntax.\\nEach top-level function and class in the code is loaded into separate documents.\\nFurthermore, an extra document is generated, containing the remaining top-level code\\nthat excludes the already segmented functions and classes.\\nThis approach can potentially improve the accuracy of QA models over source code.\\nCurrently, the supported languages for code parsing are Python and JavaScript.\\nThe language used for parsing can be configured, along with the minimum number of\\nlines required to activate the splitting based on syntax.\\nExamples\\nfrom langchain.text_splitter.Language\\nfrom langchain.document_loaders.generic import GenericLoader\\nfrom langchain.document_loaders.parsers import LanguageParser\\nloader = GenericLoader.from_filesystem(\\n    \"./code\",\\n    glob=\"**/*\",\\n    suffixes=[\".py\", \".js\"],\\n    parser=LanguageParser()\\n)\\ndocs = loader.load()\\nExample instantiations to manually select the language:\\n… code-block:: python\\nfrom langchain.text_splitter import Language\\nloader = GenericLoader.from_filesystem(“./code”,\\nglob=”**/*”,\\nsuffixes=[“.py”],\\nparser=LanguageParser(language=Language.PYTHON)\\n)\\nExample instantiations to set number of lines threshold:\\n… code-block:: python\\nloader = GenericLoader.from_filesystem(“./code”,\\nglob=”**/*”,\\nsuffixes=[“.py”],\\nparser=LanguageParser(parser_threshold=200)\\n)\\nLanguage parser that split code using the respective language syntax.\\nParameters\\nlanguage – If None (default), it will try to infer language from source.\\nText: Step Component Descr iption\\nthat have already been loaded into the kernel to create additional steps.\\nThis is similar to how ChatGPT, Bing, and Microsoft 365 Copilot combines\\nplugins together in their experiences.\\n2.3 Connectors To get additional data or to perform autonomous actions, you can use out-\\nof-the-box plugins like the Microsoft Graph Connector kit or create a\\ncustom connector to provide data to your own services.\\n2.4 Custom\\npluginsAs a developer, you can create custom plugins that run inside of Semantic\\nKernel. These plugins can consist of either LLM prompts (semantic\\nfunctions) or native C# or Python code (native function). This allows you to\\nadd new AI capabilities and integrate your existing apps and services into\\nSemantic K ernel.\\n3 Response Once the kernel is done, you can send the response back to the user to let\\nthem know the process is complete.\\nTo make sure all developers can take advantage of our learnings building Copilots, we\\nhave released Semantic K ernel as an open-source project  on GitHub. T oday, we\\nprovide the SDK in .NET and Python flavors (T ypescript and Java are coming soon). For a\\nfull list of what is supported in each language, see supported languages .\\nSemantic Kernel is open-source\\n'}\n",
      "\n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"The Semantic Kernel supports code parsing for two programming languages: Python and JavaScript. By using the respective language syntax, the code can be split into separate documents, with each top-level function and class loaded into its own document. Additionally, an extra document is generated containing the remaining top-level code that excludes the segmented functions and classes.\\n\\nHere are the supported languages for code parsing in the Semantic Kernel:\\n- Python\\n- JavaScript\\n\\nPlease note that this information is based on the available documentation for the Semantic Kernel. Let me know if you need any further assistance!\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\" Assistant is a large language model designed to help employees of a company to find answers to their questions for various \n",
    "technical topics.\n",
    "You have access to an Azure Cognitive Search index that contains a large amount of technical documentation on different topics.\n",
    "You can search for the asked information in the index and return the most relevant results to the user. \n",
    "The index contains information about the following topics: Semantic Kernel, Langchain, and others.\n",
    "You are designed to be an interactive assistant, so you can ask the user for more information if needed to find the most relevant answer.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"Which programing languages are supported by semantic kernel?\"}]\n",
    "    \n",
    "functions = [\n",
    "        {\n",
    "            \"name\": \"get_info\",\n",
    "            \"description\": \"get detailed information about asked topic from Azure Cognitive Search\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"topic\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"the domain of the question being asked, such as the semantic kernel, langchain, etc.\",\n",
    "                    },\n",
    "                    \"query\": {\n",
    "                        \"type\" : \"string\",\n",
    "                        \"description\": \"the question that was asked\",\n",
    "                        \n",
    "                    },\n",
    "                    \"topic_filter\": {\n",
    "                        \"type\" : \"string\",\n",
    "                        \"description\": \"\"\"The filter to apply for the topic. Generate the filter using the following format: topic/any(i: i eq 'Semantic Kernel')\n",
    "                        If you are not sure about the topic, generate empty filter: topic/any(i: i eq '')\"\"\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"topic\",\"query\",\"topic_filter\"],\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "available_functions = {\n",
    "            \"get_info\": get_info,\n",
    "        } \n",
    "\n",
    "\n",
    "result = run_qna(messages, functions, available_functions)\n",
    "print(result['choices'][0]['message'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
